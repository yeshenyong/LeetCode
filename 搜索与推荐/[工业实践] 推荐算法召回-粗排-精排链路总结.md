# 推荐算法召回-粗排-精排链路总结

摘自：https://zhuanlan.zhihu.com/p/463021052

值得一看

摘要

### 名言

> **召回决定天花板，粗排为了性能效率，精排决定最终推荐精度**



> 从召回到精排，每一层漏斗其实都是有损失的，而这个损失是因为现有算法工程限制



> 在召回的评价指标更着重于hitrate，粗排考虑auc/gauc/ndcg，精排考虑auc/gauc。要想做出**效果必须从上至下去优化**，假如只是优化召回多样性，但粗排没有相应的特征召回的信息是无法透传到精排



> **推荐算法其实是需要工程和业务共同努力，不是仅仅靠怼特征，魔改模型就能够出效果**。没有好的工程系统，算法业务的发展会严重受限（如良好的推理框架，训练集群，离线平台，内存数据库等）

### 负采样方式：

deepmatch虽然在召回队列的quota只有10%左右，但**曝光占比却去到60%**。因为deepmatch在召回又举足轻重的地位，因此这里另外加多一节去简要说明其常用做法。

| 采样方式                 | 说明                                                         |
| ------------------------ | ------------------------------------------------------------ |
| 全局随机负采样           | 这种随机从全场景曝光过item采样，使用listwise存储负样本，能够最大程度保证数据分布一致，但随机采样的负样本有可能跟正样本差异大。对于一些hard negative sample能区分得好。 |
| in batch负采样           | 由于listwise样本存储空间大（negative sample可能上百），因此正样本内做batch内负采样，即batch内的user vector与item vector构建一个cosine对角线矩阵，对角线为正样本的cosine，其余作为负样本cosine。这种采样方式是有损的，但实验对比在可接受范围内，而且负样本都是其他正样本，因此具有一定热度打压的作用。 |
| Popularity随机选择负采样 | 基于随机负采样，加入热度item作为负样本。因为热门item没有作为正样本，那么极有可能该item是不相关或者用户不感兴趣。 |
| Hard负采样               | 模型在训练/serving时，总有部分item逃过模型的法眼，透传到粗排甚至精排当中。**因此可以通过线上日志中找出有召回但粗排过滤的，有召回但没有曝光。又或者在训练过程当中，从item库中检索cosine值高于某一个theshold的items，并随机选取**。此举可以提高模型的精度，过滤无关的item。 |





















