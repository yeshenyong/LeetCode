# [工业实践-自我撰写] 连续特征的Embedding



当前大多数的研究主要集中在设计更复杂的网络架构来更好的捕获显式或隐式的**特征交互**，如Wide & Deep的Wide部分、DCN中的CrossNet、DIN中的注意力机制等等。而另一个主要的部分，即**Embedding模块**同样十分重要，出于以下两个原因：

1）Embedding模块是FI模块的**上游模块**，直接影响FI模块的效果；
2）CTR模型中的大多数参数集中在Embedding模块(巨大的embedding table！)，对于模型效果有十分重要的影响。

但是，Embedding模块却很少有工作进行深入研究，特别是对于连续特征的embedding方面



CTR预估模型的输入通常包含连续特征和离散特征两部分。对于离散特征，通常通过embedding look-up操作转换为对应的embedding (之后我会介绍谷歌对离散特征embedding的改进)；而对于连续特征的处理，可以概括为三类：No Embedding, Field Embedding和Discretization（离散化）

## No Embedding

No embedding是指不对连续特征进行embedding操作，而直接使用原始的数值。如Google Play的Wide & Deep直接使用原始值作为输入;而在Youtube DNN[[1\]](https://zhuanlan.zhihu.com/p/415610118#ref_1)中，则是对原始值进行变换（如平方，开根号）后输入：

![img](https://pic1.zhimg.com/80/v2-f377e77b4c542178d32abd93d9861c28_1440w.webp)



这类对连续特征不进行embedding的方法，由于模型容量有限，通常难以有效捕获连续特征中信息。

## Field Embedding

Field Embedding是指同一个field无论取何值，都共享同一个embedding，随后将特征值与其对应的embedding相乘作为模型输入：

![img](https://pic2.zhimg.com/80/v2-17a7c017d37624d7fed97dacc98bd60d_1440w.webp)

由于同一field的特征**共享**同一个embedding，并基于不同的取值对embedding进行**缩放**，这类方法的表达能力也是有限的



## Discretization

Discretization即将**连续特征进行离散化**，是工业界最常用的方法。这类方法通常是两阶段的，即首先将连续特征转换为对应的**离散值**，再通过**look-up**的方式转换为对应的embedding

首先探讨一个问题，为什么需要对连续特征进行离散化呢？或者说离散化为什么通常能够带来更好的效果呢？关于这个问题的探讨，可以参考知乎问题：

> 首先探讨一个问题，为什么需要对连续特征进行离散化呢？或者说离散化为什么通常能够带来更好的效果呢？关于这个问题的探讨，可以参考知乎问题：
>
> 1. 离散特征的增加和减少都很容易，易于模型的快速迭代；
> 2. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
> 3. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
> 4. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
> 5. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
> 6. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
> 7. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
>
> 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。



总的来说，将连续特征进行离散化给模型**引入了非线性**，能够**提升模型表达能力**，而对于离散化的方式，常用的有以下几种：

1） EDD/EFD (Equal Distance/Frequency Discretization)：即等宽/等深分箱。对于等宽分箱，首先基于特征的最大值和最小值、以及要划分的桶的个数  ，来计算每个样本取值要放到哪个箱子里。

对于等深分箱，则是基于数据中特征的频次进行分桶，每个桶内特征取值的个数是大致相同的。

2）LD (Logarithm Discretization)：对数离散化，其计算公式如下：

![img](https://pic2.zhimg.com/80/v2-217616388385baee2844c4a75dd9d9ed_1440w.webp)

3）TD (Tree-based Discretization)：基于树模型的离散化，如使用GBDT+LR来将连续特征分到不同的节点。这就完成了离散化。

**离散化方法的缺点：**
1）TPP (**T**wo-**P**hase **P**roblem)：将特征分桶的过程一般使用启发式的规则（如EDD、EFD）或者其他模型（如GBDT），无法与CTR模型进行一起优化，即**无法做到端到端**训练；
2）SBD (**S**imilar value **B**ut **D**is-similar embedding)：对于边界值，两个相近的取值由于被分到了不同的桶中，导致其embedding可能相差很远；
3）DBS (**D**is-similar value **B**ut **S**ame embedding)：对于同一个桶中的边界值，两边的取值可能相差很远，但由于在同一桶中，其对应的embedding是完全相同的。

![img](https://pic4.zhimg.com/80/v2-ea035ad5a63b704bcc6938dce9923ad7_1440w.webp)





参考：[KDD2021 | 华为AutoDis：连续特征的Embedding学习框架 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/415610118)