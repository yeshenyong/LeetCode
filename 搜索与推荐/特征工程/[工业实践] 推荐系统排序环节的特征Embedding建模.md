# 推荐系统排序环节的特征Embedding建模

转载自：[推荐系统排序环节的特征Embedding建模 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/573576653)

> SENet 在微博embedding 侧大放光彩，但后续的优化就有点tricky 了，不同场景是否该引入那么高复杂度的模型，是有待考究的
>
> 总结
>
> 1. 卡门槛（简单，门控）
> 2. 挤水分（变长embedding）
> 3. 补营养（增强表征能力，也可以是数据侧）

随着深度学习在推荐系统应用的发展，特征Embedding建模日益重要，**同时海量特征的稀疏性及模型参数量过大是必须面对的难题**。今天会为大家分享**微博在特征Embedding建模方向**做的一些工作

今天的介绍会围绕下面五点展开：

- **特征建模的必要性**
- 特征建模的三个技术方向
- 卡门槛：**微博在特征重要性方向的工作**
- 挤水分：变长特征Embedding
- 补营养：提升**特征表达质量**



### **特征建模的必要性**

**三大AI方向的领域特点**

![img](https://pic4.zhimg.com/v2-340c419ff53aa637f2a36458d6734eaf_r.jpg)

在讨论特征建模之前，我们先来对比一下不同AI应用领域中数据分布的特点，这里以最常见的自然语言处理、图像处理和推荐系统为例来说明。
首先，我们对比自然语言处理和图像处理：NLP最基本的数据元素是单词，每个单词有一定的含义，可能指代某个实体；图像处理最基本元素是像素，因为粒度太细，单个像素并没有实际含义。所以在输入信息基本单元粒度上，自然语言处理比图像处理更抽象，更有具体含义和所指。
在数据的组织结构上看，自然语言处理的语句是**一维线性**的，同时具备信息的**局部相关性**和**远程相关性**特点：局部相关性指的临近单词之间语义相关度高，远程相关性举个例子：“虽然“….”但是”是个表达语义转折关系的结构，但是它们在句中的距离可能很远，但是这是个重要特征。而图像是**二维/三维**立体结构，数据具有**局部相关性**和**平移不变性**等特点：所谓“局部相关性”，就是刚才说的，单个像素无含义，但是相邻一片区域的像素连在一起就能体现物体特征。平移不变性举个例子，比如上图里的鸟头，不论这个鸟头出现在图片哪个位置，你都应该看出来它是个鸟头，这就是平移不变性的含义，除此外还有旋转不变性、遮挡不变性等很多特性。
在这些数据的领域特点下，我们回顾这两个领域模型的发展历史，你会发现RNN结构天然匹配NLP数据的一维线形以及局部和远程相关性等特性，所以这是为何几年前深度学习进入NLP的时候，RNN快速普及的根本原因。图像处理里CNN则天然适合对局部相关性和各种不变性建模，这也是为何CNN快速占领图像领域的根本原因。**从这里可以看出，我们在设计模型的时候，应该重点考虑领域数据的特性，那些匹配领域数据特性的模型，就容易取得优势。**目前Transformer统一了NLP和图像处理领域，那么很自然，我们就可以推论：Transformer不仅匹配上述两个领域的数据特点，而且比RNN和CNN匹配度应该更高。
我们主题是推荐系统，那么我们再来看推荐领域数据分布有什么独有的特点。首先，和NLP及图像处理不同，推荐领域数据是**异质表格数据**，一个实例一般由离散的字段构成，字段可以是数值型的，也可以是离散形态的，字段之间可能有关系，也可能完全无关。其次，推荐数据输入特征之间**没有局部相关性**，对于平铺特征类型的任务，并不是两个特征在输入里挨得越近，就越有关联，它们之所以挨在一起，完全是随机作用（当然，行为序列具有局部相关性）。再者，可能也是推荐数据最大的特点，是特征的**海量高维稀疏**，关于它的具体含义我们后面会说。另外，**特征组合**对于推荐来说是至关重要的数据特点，这个**可以理解为远程相关性**。这些都是推荐领域数据独有的特点。那么，**什么样的模型才匹配推荐领域的数据特点呢？**这也是我自己一直在思考的问题，我们做的一些工作可以看作对这个问题的部分回答。

### **2.推荐系统数据的稀疏性** 

**![img](https://pic4.zhimg.com/80/v2-8b4c3dd6464840485de055dde6a04fef_1440w.webp)**


推荐领域特征的海量稀疏性，主要体现在用户行为及特征数据分布极不均匀，极少量高频特征在数据总体中占比很高，长尾现象异常严重，对于绝大多数行为特征或其它特征来说频次极低，所以就没什么信息量，或者包含大量噪音数据，这就给上层模型建模带来很大难度。

### **3.特征建模的必要性**

![img](https://pic1.zhimg.com/80/v2-0eeea706b3512611656348fee373310c_1440w.webp)

在介绍后续内容前，我们先简单介绍下DNN模型，因为后面很多内容都要以它为基础。上图展示了典型的DNN模型结构：输入实例由n个特征构成，每个特征由one-hot形式转换成特征Embedding，这个映射过程需要学习获得，然后特征Embedding concat到一起输入到上层DNN结构，DNN结构一般由2到3层MLP构成。DNN模型是最简单的深度CTR模型，也是大多数其它深度CTR模型的关键组件，但是如果你愿意好好调试，会发现**在推荐领域，DNN是性能很好的强baseline模型，大量所谓新模型，效果未必能打过它。**

![img](https://pic3.zhimg.com/80/v2-df1c8eca6f97371989c7c3874078d73e_1440w.webp)

我们从参数占比来说明为何特征建模是重要的，具体网络结构配置数据可参考上图，在这个配置下，你把计算结果列出来一看就明白了：在DNN结构中，包含200亿参数量，特征Embedding占总体参数量的93.3%，上层MLP结构参数量只占6.7%。也就是说，**深度CTR模型里，特征Embedding占据了参数总量的绝对多数**，理论上特征处理好坏对模型整体影响应该比较大。在真实应用场景中，**深度CTR模型一般都是“过参数化”（over-parameter）的**：模型参数的规模远远超过训练数据规模，比如上面200亿的模型，大概率你没那么多训练数据来训练它。而**深度模型这种天然的“过参数化”倾向导致它天然是个容易引起“过拟合”问题的模型结构**，而造成这种现象的，主要原因是占比大的特征Embedding。当然你可以调整上层MLP大小，比如增加MLP的深度和宽度，但是这样做尽管对模型效果有影响，但是影响很小。所有这些现象或特点，主要原因在于深度模型中包含了海量的特征Embedding参数量。

![img](https://pic3.zhimg.com/80/v2-aef79e69d17c16b5eeeca5d4c5f4341a_1440w.webp)

特征参数量虽然看着非常巨大，容易引起过拟合问题，但是，其中大量特征Embedding其实是没什么用的。你可以试着逐步抛掉低频特征，在实验中可以发现，开始把极低频特征比如仅仅出现1次2次的特征抛掉，模型效果会逐渐上升，到了一定数量比如出现10次的特征也抛掉，模型效果就开始下降。这说明对于大量低频特征，引入它们的副作用要大于正向作用。如果我们能知道哪些特征没用，把它们抛掉，无疑不仅能提升模型效果，还能大规模减少模型参数，一举两得。但是问题的关键是：面对如此大量特征，我们并不知道哪些是有用或者无用的。所以，**特征建模的核心问题就是：我们能否通过技术手段，知道哪些特征是有用的？哪些是没用的？对于模型认为有用的特征我们可以做些什么事情？模型认为没用的特征，我们又可以做些什么事情？**

## **▌特征建模的三个技术方向**

![img](https://pic4.zhimg.com/80/v2-dfb7f7bd2acc18fbd79aa79e15cef04f_1440w.webp)

这个部分，会简单说明下特征建模三个方向的基本思路，我们首先来看“**卡门槛**”。所谓“卡门槛”，意思是我们可以在特征Embedding层和上面的DNN层之间，加一个门控层，这个门控来决定哪些特征可以进入上层的DNN部分，哪些特征不能进入后续网络结构，直接在门控层就被过滤掉。如果进一步思考，其实对于允许通过的特征，这个门控机制还可以区分不同特征的重要性，对于重要特征给予大权重，不那么重要特征给予小的权重。这就是“卡门槛”的基本思路，它的**核心思想是过滤掉对优化目标起负面作用的大部分稀疏特征，同时通过大权重凸显重要特征的作用**。如果用“教育学生学习”来打个比方，类似我们现在的高考制度，划出统一分数线，只有过线的学生才能通过高考去上大学。

![img](https://pic3.zhimg.com/80/v2-1278ac2190bc660f4f3d439889a44ca6_1440w.webp)

第二个方向是“**挤水分**”。它的含义是：通常我们会给所有特征分配相同长度的Embedding size，但是很明显这么做是不合理的。因为对于大量稀疏特征，因为没有什么知识可以学习，如果你给它分配的Embedding size越大，它越容易出现过拟合的现象；而对于那些高频特征，你应该给它更长的Embedding，因为它有很多知识需要编码，如果分配的太短，则会出现欠拟合问题，就是Embedding容量不够大，放不下那么多要学习的知识。所以，**合理的策略应该是：对于特征采取变长Embedding，对于中高频特征，分配长的Embedding，对于大量低频特征，分配短的Embedding**。这就是在挤稀疏特征Embedding的水分，这是我把它称为“挤水分”的原因。如果继续拿上面的学生教育做比喻，那么“挤水分”就类似学校里面给不同学生设置快慢班，学的快的学生进快班，获得更好的教育资源，学的慢的学生进慢班，因材施教。

![img](https://pic2.zhimg.com/80/v2-d3adea56d83cd3ff50c91021d275f005_1440w.webp)

第三个方向是“**补营养**”。这是一种最正能量的技术方法，它的意思是：既然很多稀疏特征，它没太多可学习的内容，导致特征稀疏，表达能力不足，那么我就想点办法，来提升它的特征表达能力。具体做法又可以细分为“修炼内功”和“寻求外援”，这里暂不展开，后面会用具体例子解释这两种做法的思路。如果继续拿学生教育做类比，那么“补营养”类似学校里面专门给差生进行针对性的补课，来提高他们的学习成绩，所以说这是最“正能量”的一种做法。

![img](https://pic2.zhimg.com/80/v2-dea231a6cda8a63e5ed57aa2546464c9_1440w.webp)

上述内容是特征建模的主要技术方向，接下来分享一些我个人的想法。我们最近4年来，在排序模型方面，重心放在特征建模方向，尤其是聚焦在“特征重要性”方面，可以说这是我们做Rank模型的核心主线。那么，为什么我们在这里投入这么大精力呢？首先，前面介绍过，我们做模型，一定要结合具体领域的业务特性，以及领域内数据的独有特点。具体到推荐系统，我个人认为**海量高维的稀疏特征以及特征组合，这两个是推荐领域最主要的领域特点**，所以我们把重心放在特征建模这个方向。其次，我认为**相比模型结构来说，特征建模更重要**。我一直相信的一点是：尽管最近一年来图像和NLP领域已经被transformer统一，推荐系统暂时没有类似这样有统治力的模型，但是**如果未来某天出现了这种突破性的模型，我觉得应该是在特征建模方向，而不是模型结构的改进和革新。**

那为什么我们又聚焦在特征重要性方向上呢？原因是在2019年之前，整个行业就没人做“卡门槛“这个方向，**FiBiNet中的SENet模块应该是第一个做这个事情的CTR模型**。有些介绍文章把FiBiNet里的SENet仅看作一种Attention具体做法，也有拿AFM模型放在一起来说的，我认为这种理解是片面的，这并不能体现引入SENet的本质思想，关键你要看它用在哪里，目的是什么，具体怎么做其实是次要的，上面介绍的“卡门槛”才是它的本质思想。

## **▌卡门槛:微博在特征重要性方向的工作**

![img](https://pic1.zhimg.com/80/v2-a9f2db5c864b93019266ebc6675f2f58_1440w.webp)

这里对最近几年我们在特征建模方面做的工作做个概述，节奏大约是一年提出1到2个新模型。最近几年微博在Rank模型方面做的主要工作如下：

- FiBiNet是第一个引入特征重要性子网络结构的深度CTR模型，也是目前效果最好的CTR模型之一；
- FiBiNet++是对FiBiNet模型的改进，主要目的在于减少bi-linear模块设计不合理带来的暴增的参数数量；
- ContextNet是对transformer模型进行改造，将之应用在推荐的改进模型，在特征重要性模块加入了更精细的特征门控方式，这是我个人比较喜欢的一个模型；
- MaskNet是ContextNet的副产品，将特征门控拓展应用到MLP上。这是除了我们今年做的一个还未公开的新模型外，我们自己做的模型里效果最好的。

今天主要介绍上述几个模型，至于FAT-DeepFFM和GateNet，也是两个特征门控相关工作，时间原因今天就不介绍了，感兴趣的同学可以搜对应论文看看。

![img](https://pic4.zhimg.com/80/v2-7f285ebc167d323f3388cb9ee41a8893_1440w.webp)

![img](https://pic1.zhimg.com/80/v2-4c04e4eb621ab035cb3fe933c587380c_1440w.webp)

FiBiNet是我们18年设计、发表在Recsys2019的工作。相对其它模型，它的创新点在于引入两个独立可插拔的模块:一个是用SENet作为特征重要性门控，二是引入双线性模块来改进特征交互方式。这对应下图网络模型结构下方两个分支中标为橙色的三个模块。

![img](https://pic4.zhimg.com/80/v2-6d1f414444a38721c2f81020e48700b3_1440w.webp)

SENet最早2018年在图像处理中提出的，下面介绍一下它在推荐系统中的应用。

![img](https://pic2.zhimg.com/80/v2-82f774916bff27da249b2df8260ef099_1440w.webp)

我们在推荐中引入SENet的主要目的是希望引入一个特征门控系统，它可以给输入的每个特征动态打权重，如果是没什么用的稀疏特征，我们希望这个权重是0，这等于消除掉了这个特征的负面影响，而如果是重要特征，则希望SENet打出一个大权重，强调下这个特征很重要。基本出发点就是之前讲过的“卡门槛”的作用。

具体而言，其模型结构参考上图，对于当前输入的每个特征Embedding，取出一个代表bit位，这里取Embedding中根据每一位求出的均值作为代表位，假设输入有f个field或slot，则有f个代表bit，作为后续双层MLP网络的输入。第一层MLP是个窄网络，因为输入维度f一般比较小，实际场景中基本也就是100到200之间，所以这层MLP可以设计的比较窄，就不容易过拟合；第二层MLP的输出结果就是每个特征的重要性得分。我们原始版本的SENet中，非线性函数都是用的Relu，你会发现很多特征打出来的权重都是0分，这样就达到了前面说的“卡门槛”的目的。从网络结构可以看出，**SENet是个轻型网络，额外引入的参数很少，运算速度也比较快，作为一个可插拔的门控模块是比较轻巧的。**

![img](https://pic3.zhimg.com/80/v2-735686dc6eeefbc375d8d517586dce36_1440w.webp)

另外一个是双线性特征交互模块，这是我们18年在改造FFM模型时候想出来的改进模型，是对FFM模型的简化或者是FM模型的复杂化，看你站在哪个角度来看了。**双线性交互的核心思想是：在FM模型基础上，当任意两个特征交互的时候引入一个参数矩阵W，通过这个参数矩阵来更精细地表征特征交互过程。**

![img](https://pic1.zhimg.com/80/v2-c7f968a91f6ba50e8995737fb9681d78_1440w.webp)

这个参数矩阵W，有三种做法：一种是所有特征共享同一个W,这种方式的W引入的新参数量是极小的，几乎可忽略不计 ；第二种是每一个特征域（Field或Slot）内的特征共享同一个W;第三种最细致，是每种特征域组合，共享一个W。这三种不同做法，越来越精细，表达能力越来越强，对应的参数量依次升高，跑起来也会越来越慢，具体用哪个要看具体场景。关于双线性这里不展开介绍了，对此感兴趣的同学，推荐看看小红书王树森老师在B站的解说视频（[SENet 和 Bilinear 交叉](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1SY4y1M7bD/) ），介绍FiBiNet很到位。

![img](https://pic1.zhimg.com/80/v2-e1507c768eaf91783c0995f813ac53a8_1440w.webp)

从19年我们提出FiBiNet，再结合我参考最近两年的模型发展情况，我的结论是：**FiBiNet是目前效果最好的CTR模型之一，尤其是在Avazu数据集上**，大概率FiBiNet到目前这个时间点就是2022年，尽管有一些新模型出现，我判断它在Avazu上很可能仍然是效果最好的CTR模型（有个公平对比的前提条件：Avazu数据集上，特征Embedding size要放到40或者50，这是绝大多数CTR模型在Avazu上效果最好时候的Embedding size，而不是像很多工作实验部分报道的，强制把Avazu数据集上的Embedding size设置成10，这并非一个合理设置。Criteo数据集最合理的Embedding size应该在10到20之间）。

上面这张图是得出判断的依据之一，这是华为FuxiCTR的工作，它做了4600多组实验来公平对比从2010到2020年这10年间比较知名的24个CTR模型的效果，结果显示在Avazu上FiBiNet效果最好。当然这只是我得出上述结论的依据之一，还有其它依据，包括我们内部也对新模型做过大量实验对比，我个人觉得上述结论基本成立。

不过话说回来，最近2年我的一个明显感觉是：**CTR模型的效果和数据集关系比较大，目前应该不存在一个模型，它在各种数据集都能占据统治地位**，这和NLP以及图像处理领域表现完全不同，这两个领域基本都有统治性的模型，这也一定程度上说明了，推荐模型整体缺乏突破进展，才会造成在不同数据集上最佳模型不同，类似军阀割据的这种外在表现。而**在某个数据集上表现最好的CTR模型，很可能是因为它的某个网络结构设计，正好匹配了这个数据集合的数据分布特性。**

这里顺便再说下数据集的问题，如果你要测试CTR模型且以论文的方式呈现出来，很明显实验部分的测试数据应该带上Criteo和Avazu，因为这两个数据集规模比较大，都在4500万左右。尽管这两个数据集也有一些问题，但是貌似除此外我们也没有更好的选择，这也是没办法的办法。而如果一个论文里的CTR模型，实验部分都用的是小规模数据，很明显这个工作就没太多实用价值，因为小数据上有效的技术改进，大概率换个规模大的数据就会失效。即使在Criteo和Avazu证明有效的模型，上线未必会有效，因为你面临比4500万更大规模的数据，技术失效也不意外，但是如果一个新模型在实验部分连这两个数据都不带，都是小规模数据，那只能说明模型作者对自己的模型效果不够自信。所以如果你的出发点是真正想提出有效的改进模型而不仅仅是水一篇论文，那我建议一定要用大规模数据来验证。

![img](https://pic4.zhimg.com/80/v2-2c2c69fe4f6334c9d69f23161d05cd4b_1440w.webp)

![img](https://pic1.zhimg.com/80/v2-d03c9146adb2da4c797b4316dc120954_1440w.webp)

这里介绍下使用FiBiNet的一个经验：上面两张图片展示了不同模型在Criteo和Avazu数据集上的效果对比，坐标横轴是不同大小的特征Embedding size，纵轴是Logloss和AUC效果指标，图中的黄色曲线代表了FiBiNet的效果表现。从中可以看出，无论是Criteo还是Avazu数据集合，当逐渐放大特征Embedding size，尤其当大小超过30的时候，FiBiNet相对其它模型（DeepFM/xDeepFM/AutoInt+）的效果优势逐步扩大，而其它模型的效果要么开始往下掉，要么开始波动，只有FiBiNet随着Embedding size的继续放大，性能稳步提升。也就是说如果**应用中Embedding size设置的比较大的时候，明显使用FiBiNet效果更好。**

这是为什么呢？我觉得这个现象说明了一个问题：正是FiBiNet中的SENet模块导致这个现象的发生。因为当特征Embedding size越大的时候，稀疏问题就会越严重，而此时使用特征门控就越重要，因为此时SENet有更大可能将无效特征过滤掉，避免了大量特征稀疏的负面影响。































