# 推荐算法的奇技淫巧

> 虽说奇技淫巧，但也有一部分业界根据
>
> 以下篇幅根据业界以及实际工作中遇到的 `tricky` 的做法进行记录





**最后一层全连接层不加`bias`**

> 可以有这些说法，在某些情况下，最后一层全连接层可以不加bias ，例如在二分类任务中（ctr 任务），如果输出层只有一个神经元，那么这个神经元的输出就可以表示为权重和输入的乘积，不需要再加上bias。但是在多分类任务中，通常需要添加bias 来保证每个类别都有一个独立的bias 项，以此提高模型的表现效果。因此，在实际应用中，是否需要添加bias 取决于具体的任务和模型
>
> 业界参考代码：https://github.com/alibaba/EasyRec/blob/e80649c2b878b913c1854da0f6858a9c5c8e7473/easy_rec/python/model/pdn.py#L119



**最后一层全连接层不加`activation_fn`**

> 有时候最后一层不需要加激活函数。最后一层的输出通常被解释为模型对目标变量的预测值，经过激活函数后的输出并不一定符合预测值的定义。例如，在回归问题中，预测值通常是实数，而在分类问题中，预测值通常是每个类别的概率分布。因此，最后一层的输出通常需要根据具体问题而设置。对于回归问题，最后一层不需要加激活函数，因为预测值可以是任何实数。对于分类问题，最后一层的输出通常需要softmax 变换，将输出转换为每个类别的概率，所以创建最后一层需要指定softmax 参数为activation_fn（当然二分类也就不用）









