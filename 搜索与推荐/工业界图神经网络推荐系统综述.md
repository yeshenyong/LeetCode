# 工业界图神经网络推荐系统综述

转载自：https://zhuanlan.zhihu.com/p/423342532

## 图神经网络的优势

在应用某项技术解决业务场景中的某个问题时，我们需要充分了解这项技术的特点和优势，以下从五个方面谈谈个人对GNN优点的理解。

- **GNN VS MLP/CNN/RNN**：图数据中结点邻居具有两个特点，一是**数量不定**，二是**顺序不定**，因此MLP/CNN/RNN无法直接处理这样的非欧式数据而只能用GNN建模。实际上，我们可以将GNN看做一种更加泛化的模型，例如，RNN相当于线性图上的GNN，而Transformer相当于完全图上的GNN。

- **GNN VS Graph Embedding**：在GNN火起来之前已经涌现出很多Graph Embedding方法，并被广泛应用在搜推的向量召回阶段，这类方法受**Word2vec**[[30\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_30)启发设计，从最初的的**Item2Vec**[[31\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_31)的Item Sequence+Skip-Gram，到**DeepWalk**[[32\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_32)的Random Walk+Skip-Gram；到**Node2Vec**[[33\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_33)基于平衡同质性和结构性的考虑改进Random Walk部分；到**MetaPath2Vec**[[34\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_34)基于对图的异构性的考虑改进Random Walk部分；到**EGES**[[35\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_35)引入属性数据缓解行为数据的稀疏性，可**以发现这类方法都遵循着Skip-Gram的范式**。GNN相比这些方法的优点主要体现在四处：

- - **GNN可以结合目标任务端到端地训练**，而Graph Embedding更像是**预训练**的方式，其学习到的Embedding不一定与我们的**目标任务相关**，特别是在**样本规模庞大**的业务场景，端到端训练得到的Embedding比预训练得到的Embedding更有效。
  - **GNN的层级网络结构方便与其他深度学习技术结合（**缝合怪水论文最爱），例如GCN+Attention=GAT。
  - **GNN可以适用Inductive的任务**，即当图的结构发生变化后，例如加入了一些新的结点，Graph Embedding方法就需要重新训练模型，而GNN可以使用类似GraphSage Node-Wise Sampling的方式，使用已经训练好的模型直接对新的结点进行推断。
  - **GNN可以使用更加丰富的特征**，Graph Embedding方法本质上使用的是ID特征，GNN在消息传递的过程中可以使用多种特征。

- **GNN VS Feature Concat & Collaborative Filtering & Proximity Loss**：GNN相比后三种方法的优点可以统一归纳为：**通过堆叠多层显示地学习高阶的关联信息**。**Feature Concat**表示将特征拼接到一起然后通过特征交叉（例如FM，NFM等）可以学习到**一阶的属性关联信息（**区别于交叉特征的阶数**）**，例如，user a买过item b，item b和item c都具有属性attribute a，那么user a也有可能购买item b，但是Feature Concat不保证能学到**高阶的属性关联信息；Collaborative Filtering**可以通过用户历史行为学习到**一阶的行为关联信息**，例如，user a和user b都购买过item a， user b又购买过item b，那么user a也有可能购买item b；**Proximity Loss**表示在损失函数中加入正则项使得相邻的结点更相似，但是一方面它是一种隐式的方式，另一方面想确保学习到高阶的相似关系，就需要加入更复杂的2,3，...，K阶正则项，实际上这也是GCN提出时的出发点之一。

## 论文总结

该章节对选取的工业界的文章的**共性部分**进行总结，除了有人比较喜欢用来水论文的模型结构也涉及了图的构建，特征使用，采样方法，结合方式等部分。可以看到，对GNN的应用基本遵循着这套框架。

### 2.1 应用阶段

推荐系统不同阶段的特点影响着我们对某项技术的使用，召回阶段可以说是样本的艺术，而排序阶段可以说是特征的艺术。其中向量召回是一类常用的个性化召回方法，一般在离线训练时存下User和Item的Embedding，线上推断时通过LSH等方法从海量候选集中快速选出用户可能感兴趣的Items。以下总结了召回阶段常见的几个特点：

- 召回模型一般不会使用太多复杂的特征，以ID特征为主；排序模型会上很多特征尽可能描述用户，物品及行为过程。
- 召回模型一般使用PairWise Loss，排序模型一般使用PointWise Loss。个人理解一方面是因为召回阶段的目标是筛选出用户可能感兴趣的Item，至于感兴趣的程度是多少那是排序模型的职责，因此**只需要使用PairWise Loss将正负样本尽可能区分开**即可。另一方面是因为召回阶段的负样本不一定表示用户不感兴趣，只是没有曝光而已，如果**用PointWise Loss建模会导致模型受噪声的干扰**。
- 召回模型一般要从**全库随机选取负样本**，排序模型一般将**曝光未点击作为负样本**。在训练召回模型时时将曝光未点击作为负样本存在两个问题，一是**线下线上的不一致**，**线上召回时面对的是全库的候选集**；二是**在上一轮能够得到曝光的物品已经属于用户比较感兴趣的**，只不过同时曝光的还有更符合用户需要的选项，将这些样本直接作为召回模型的负样本不太合适。这里的“**全库”也会根据场景变化**，例如在搜索场景，**由于Query的相关性限制**，所以会在同类目下采样负样本。

**GNN由于其构图，采样和计算的复杂性，更多被应用在召回阶段做向量召回**。常见的一种方式是将Item推荐建模为**User结点与Item结点的链接预测任务**，同样在离线存下训练好的User和Item Embedding用于线上召回。不过在建模链接预测任务时，**很容易产生信息泄露的问题，**即在做消息传递时，**没有将待预测的边从图中去掉**，例如预测user a对item a是否感兴趣，没有去掉图中两者之间的边，user a和item a作为邻居直接融合了彼此的Embedding，**导致模型难以学习到有效的信息**。在复现一些论文的代码时，我发现这个问题还挺常见的。当然在召回阶段我们也可以**结合目标任务端到端地训练**。GNN也可以应用在**排序阶段**[[36\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_36)[[37\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_37)[[38\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_38)[[39\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_39)[[40\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_40)[[41\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_41)[[42\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_42)[[43\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_43)，此时存在两种结合方式，一种是先**预训练**[[42\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_42)，得到的Embedding以**特征初始化**或**Concat**的方式辅助排序模型的训练，另一种是GNN模块与排序模型整体一起做**端到端地训练**，不过这样**需要考虑到线上打分时的效率，特别是GNN采样以及聚合带来的开销。**当然我们可以**将GNN模块作为Embedding Layer的一部分**，在离线训练时得到包含了图信息的Embedding，**在线上打分时直接使用该Embedding而无需调用GNN模块**。



### 2.2 图的构建

“Garbage in, garbage out”，图数据构建不好，GNN魔改得再花哨也难奏效。对于构建图的数据，从**数据来源**来看，分为**行为数据，属性数据和社交数据**；从**时间跨度**来看，分为**短期数据和长期数据**；从**用户粒度**来看，分为**单个用户**和**群体用户**；不同种类的数据构建的图蕴含着不同的特点，下面一一介绍。

- **行为数据**：行为数据是搜推广场景最常见也最重要的一类数据，应用很广的行为序列建模就是建立在该数据上，详情可以参考之前写的一篇文章：[没什么大不了：浅谈行为序列建模](https://zhuanlan.zhihu.com/p/420995638)。该数据可以构建两种类型的图：

- - **二分图**：最常见的方式是使用行为数据直接构建**User-Item二分图**，在user和其行为过的Item之间构建边，不过二分图的**1阶邻居往往非常稀疏**，因此有工作**通过二分图的2阶邻居分别导出User-User和Item-Item同构子图**[[39\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_39)，一方面通过**2阶邻居的丰富性**缓解了1阶邻居的稀疏性，另一方面也**避免了对异构图的复杂建模**，可以直接在子图上使用同构GNN。User-Item二分图的另一个缺点是**难以及时反映用户的新的行为（即需要考虑图的动态性）**。
  - **共现图**：共现关系表达了物品之间的关联，一方面可以在**行为序列相邻的Item之间构建共现邻居关系**[[36\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_36)，前后行为的Item一般比较相关；另一方面对于部分场景例如搜索场景，**可以在某个Query下点击过的Item之间构建共现邻居关系**，这些Item一般也比较相关。在这一过程中我们还可以统计**共现频数**[[44\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_44)，共现频数一方面可以用来**去噪**，共现频数较低的两个Item相关程度也低；另一方面可以用来**计算权重分布用于Node-Wise采样**，相比GraphSage随机采样，可以最大程度保留有效信息；对于计算的权重分布还可以用于**指导对邻居的聚合过程**。值得注意的是，在由User-Item二分图导出User-User或Item-Item子图时也可以统计类似的共现频数。

- **属性数据**：**行为数据构建的图往往是比较稀疏的**，因此可以引入属性数据构建**属性关系**[[45\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_45)。例如，Item a和Item b都具有属性Brand a，即两个商品都是同一个品牌的，这是我们可以引入**Entity结点**Brand，然后在Item a，b与Brand a之间构建属性邻居关系。这里让人不禁疑问**为什么不直接将Brand作为Item的特征呢**（Feature concat）？在上文讨论图神经网络的优点时已经提到，将Brand作为图的一部分可以用多层GNN**学习高阶的属性关联信息**。此外，当我们用属性数据与行为数据**共同构建一张更复杂的异构图**，此时还可以用GNN学习到异构的**复合关联信息**。

- **社交数据**：我们还以用社交网络进一步丰富User之间的邻居关系，不过对于盲目使用社交数据的有效性我是存疑的。**具有社交关系的人真的存在相似的偏好吗**？首先，**不同的社交关系含义不同**，例如，亲戚关系更多表示血缘上的联系，不足以表达偏好上的关联。其次，**社交关系表达的关联真的适用于我的场景吗**？例如，朋友关系表达的更多是观点或思想上的关联，在电商场景下一对朋友不一定对商品拥有相似的偏好，但是在内容场景下例如抖音上，我和朋友确实都喜欢刷猫猫狗狗的视频。

- **短期数据 & 长期数据**：对于行为数据，我们可以用第T-1的数据构建图用于第T天，也可以用连续N天的数据构建图用于第T天。短期数据更容易**保留最近的流行趋势**，例如，这两天人们抢着买压缩饼干啥的，但是构建的图会非常稀疏；长期数据更容易保留**稳定的一般规律**，例如，人们买完手机过阵子又买手机壳钢化膜啥的。

- **单个用户 & 群体用户：**单个用户的行为数据构建的图更具**个性化**[[43\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_43)，所谓“一人一图”，但是同样会存在稀疏问题；群体用户的行为数据构建的图更具**泛化性**，并且可以缓解某些长尾物品的**冷启动问题**。

以上几种模式并不是孤立的，可以根据不同的场景进行组合。此外，还存在着其他一些图模式。例如，GMCM[[38\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_38)构建的**图的结点是各种微观行为**，边是它们的转移过程，权重是其转移概率，并且将CVR预测建模为了图分类任务。

















