# 工业界图神经网络推荐系统综述

转载自：https://zhuanlan.zhihu.com/p/423342532

## 图神经网络的优势

在应用某项技术解决业务场景中的某个问题时，我们需要充分了解这项技术的特点和优势，以下从五个方面谈谈个人对GNN优点的理解。

- **GNN VS MLP/CNN/RNN**：图数据中结点邻居具有两个特点，一是**数量不定**，二是**顺序不定**，因此MLP/CNN/RNN无法直接处理这样的非欧式数据而只能用GNN建模。实际上，我们可以将GNN看做一种更加泛化的模型，例如，RNN相当于线性图上的GNN，而Transformer相当于完全图上的GNN。

- **GNN VS Graph Embedding**：在GNN火起来之前已经涌现出很多Graph Embedding方法，并被广泛应用在搜推的向量召回阶段，这类方法受**Word2vec**[[30\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_30)启发设计，从最初的的**Item2Vec**[[31\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_31)的Item Sequence+Skip-Gram，到**DeepWalk**[[32\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_32)的Random Walk+Skip-Gram；到**Node2Vec**[[33\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_33)基于平衡同质性和结构性的考虑改进Random Walk部分；到**MetaPath2Vec**[[34\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_34)基于对图的异构性的考虑改进Random Walk部分；到**EGES**[[35\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_35)引入属性数据缓解行为数据的稀疏性，可**以发现这类方法都遵循着Skip-Gram的范式**。GNN相比这些方法的优点主要体现在四处：

- - **GNN可以结合目标任务端到端地训练**，而Graph Embedding更像是**预训练**的方式，其学习到的Embedding不一定与我们的**目标任务相关**，特别是在**样本规模庞大**的业务场景，端到端训练得到的Embedding比预训练得到的Embedding更有效。
  - **GNN的层级网络结构方便与其他深度学习技术结合（**缝合怪水论文最爱），例如GCN+Attention=GAT。
  - **GNN可以适用Inductive的任务**，即当图的结构发生变化后，例如加入了一些新的结点，Graph Embedding方法就需要重新训练模型，而GNN可以使用类似GraphSage Node-Wise Sampling的方式，使用已经训练好的模型直接对新的结点进行推断。
  - **GNN可以使用更加丰富的特征**，Graph Embedding方法本质上使用的是ID特征，GNN在消息传递的过程中可以使用多种特征。

- **GNN VS Feature Concat & Collaborative Filtering & Proximity Loss**：GNN相比后三种方法的优点可以统一归纳为：**通过堆叠多层显示地学习高阶的关联信息**。**Feature Concat**表示将特征拼接到一起然后通过特征交叉（例如FM，NFM等）可以学习到**一阶的属性关联信息（**区别于交叉特征的阶数**）**，例如，user a买过item b，item b和item c都具有属性attribute a，那么user a也有可能购买item b，但是Feature Concat不保证能学到**高阶的属性关联信息；Collaborative Filtering**可以通过用户历史行为学习到**一阶的行为关联信息**，例如，user a和user b都购买过item a， user b又购买过item b，那么user a也有可能购买item b；**Proximity Loss**表示在损失函数中加入正则项使得相邻的结点更相似，但是一方面它是一种隐式的方式，另一方面想确保学习到高阶的相似关系，就需要加入更复杂的2,3，...，K阶正则项，实际上这也是GCN提出时的出发点之一。

## 论文总结

该章节对选取的工业界的文章的**共性部分**进行总结，除了有人比较喜欢用来水论文的模型结构也涉及了图的构建，特征使用，采样方法，结合方式等部分。可以看到，对GNN的应用基本遵循着这套框架。

### 2.1 应用阶段

推荐系统不同阶段的特点影响着我们对某项技术的使用，召回阶段可以说是样本的艺术，而排序阶段可以说是特征的艺术。其中向量召回是一类常用的个性化召回方法，一般在离线训练时存下User和Item的Embedding，线上推断时通过LSH等方法从海量候选集中快速选出用户可能感兴趣的Items。以下总结了召回阶段常见的几个特点：

- 召回模型一般不会使用太多复杂的特征，以ID特征为主；排序模型会上很多特征尽可能描述用户，物品及行为过程。
- 召回模型一般使用PairWise Loss，排序模型一般使用PointWise Loss。个人理解一方面是因为召回阶段的目标是筛选出用户可能感兴趣的Item，至于感兴趣的程度是多少那是排序模型的职责，因此**只需要使用PairWise Loss将正负样本尽可能区分开**即可。另一方面是因为召回阶段的负样本不一定表示用户不感兴趣，只是没有曝光而已，如果**用PointWise Loss建模会导致模型受噪声的干扰**。
- 召回模型一般要从**全库随机选取负样本**，排序模型一般将**曝光未点击作为负样本**。在训练召回模型时时将曝光未点击作为负样本存在两个问题，一是**线下线上的不一致**，**线上召回时面对的是全库的候选集**；二是**在上一轮能够得到曝光的物品已经属于用户比较感兴趣的**，只不过同时曝光的还有更符合用户需要的选项，将这些样本直接作为召回模型的负样本不太合适。这里的“**全库”也会根据场景变化**，例如在搜索场景，**由于Query的相关性限制**，所以会在同类目下采样负样本。

**GNN由于其构图，采样和计算的复杂性，更多被应用在召回阶段做向量召回**。常见的一种方式是将Item推荐建模为**User结点与Item结点的链接预测任务**，同样在离线存下训练好的User和Item Embedding用于线上召回。不过在建模链接预测任务时，**很容易产生信息泄露的问题，**即在做消息传递时，**没有将待预测的边从图中去掉**，例如预测user a对item a是否感兴趣，没有去掉图中两者之间的边，user a和item a作为邻居直接融合了彼此的Embedding，**导致模型难以学习到有效的信息**。在复现一些论文的代码时，我发现这个问题还挺常见的。当然在召回阶段我们也可以**结合目标任务端到端地训练**。GNN也可以应用在**排序阶段**[[36\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_36)[[37\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_37)[[38\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_38)[[39\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_39)[[40\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_40)[[41\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_41)[[42\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_42)[[43\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_43)，此时存在两种结合方式，一种是先**预训练**[[42\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_42)，得到的Embedding以**特征初始化**或**Concat**的方式辅助排序模型的训练，另一种是GNN模块与排序模型整体一起做**端到端地训练**，不过这样**需要考虑到线上打分时的效率，特别是GNN采样以及聚合带来的开销。**当然我们可以**将GNN模块作为Embedding Layer的一部分**，在离线训练时得到包含了图信息的Embedding，**在线上打分时直接使用该Embedding而无需调用GNN模块**。



### 2.2 图的构建

“Garbage in, garbage out”，图数据构建不好，GNN魔改得再花哨也难奏效。对于构建图的数据，从**数据来源**来看，分为**行为数据，属性数据和社交数据**；从**时间跨度**来看，分为**短期数据和长期数据**；从**用户粒度**来看，分为**单个用户**和**群体用户**；不同种类的数据构建的图蕴含着不同的特点，下面一一介绍。

- **行为数据**：行为数据是搜推广场景最常见也最重要的一类数据，应用很广的行为序列建模就是建立在该数据上，详情可以参考之前写的一篇文章：[没什么大不了：浅谈行为序列建模](https://zhuanlan.zhihu.com/p/420995638)。该数据可以构建两种类型的图：

- - **二分图**：最常见的方式是使用行为数据直接构建**User-Item二分图**，在user和其行为过的Item之间构建边，不过二分图的**1阶邻居往往非常稀疏**，因此有工作**通过二分图的2阶邻居分别导出User-User和Item-Item同构子图**[[39\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_39)，一方面通过**2阶邻居的丰富性**缓解了1阶邻居的稀疏性，另一方面也**避免了对异构图的复杂建模**，可以直接在子图上使用同构GNN。User-Item二分图的另一个缺点是**难以及时反映用户的新的行为（即需要考虑图的动态性）**。
  - **共现图**：共现关系表达了物品之间的关联，一方面可以在**行为序列相邻的Item之间构建共现邻居关系**[[36\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_36)，前后行为的Item一般比较相关；另一方面对于部分场景例如搜索场景，**可以在某个Query下点击过的Item之间构建共现邻居关系**，这些Item一般也比较相关。在这一过程中我们还可以统计**共现频数**[[44\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_44)，共现频数一方面可以用来**去噪**，共现频数较低的两个Item相关程度也低；另一方面可以用来**计算权重分布用于Node-Wise采样**，相比GraphSage随机采样，可以最大程度保留有效信息；对于计算的权重分布还可以用于**指导对邻居的聚合过程**。值得注意的是，在由User-Item二分图导出User-User或Item-Item子图时也可以统计类似的共现频数。

- **属性数据**：**行为数据构建的图往往是比较稀疏的**，因此可以引入属性数据构建**属性关系**[[45\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_45)。例如，Item a和Item b都具有属性Brand a，即两个商品都是同一个品牌的，这是我们可以引入**Entity结点**Brand，然后在Item a，b与Brand a之间构建属性邻居关系。这里让人不禁疑问**为什么不直接将Brand作为Item的特征呢**（Feature concat）？在上文讨论图神经网络的优点时已经提到，将Brand作为图的一部分可以用多层GNN**学习高阶的属性关联信息**。此外，当我们用属性数据与行为数据**共同构建一张更复杂的异构图**，此时还可以用GNN学习到异构的**复合关联信息**。

- **社交数据**：我们还以用社交网络进一步丰富User之间的邻居关系，不过对于盲目使用社交数据的有效性我是存疑的。**具有社交关系的人真的存在相似的偏好吗**？首先，**不同的社交关系含义不同**，例如，亲戚关系更多表示血缘上的联系，不足以表达偏好上的关联。其次，**社交关系表达的关联真的适用于我的场景吗**？例如，朋友关系表达的更多是观点或思想上的关联，在电商场景下一对朋友不一定对商品拥有相似的偏好，但是在内容场景下例如抖音上，我和朋友确实都喜欢刷猫猫狗狗的视频。

- **短期数据 & 长期数据**：对于行为数据，我们可以用第T-1的数据构建图用于第T天，也可以用连续N天的数据构建图用于第T天。短期数据更容易**保留最近的流行趋势**，例如，这两天人们抢着买压缩饼干啥的，但是构建的图会非常稀疏；长期数据更容易保留**稳定的一般规律**，例如，人们买完手机过阵子又买手机壳钢化膜啥的。

- **单个用户 & 群体用户：**单个用户的行为数据构建的图更具**个性化**[[43\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_43)，所谓“一人一图”，但是同样会存在稀疏问题；群体用户的行为数据构建的图更具**泛化性**，并且可以缓解某些长尾物品的**冷启动问题**。

以上几种模式并不是孤立的，可以根据不同的场景进行组合。此外，还存在着其他一些图模式。例如，GMCM[[38\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_38)构建的**图的结点是各种微观行为**，边是它们的转移过程，权重是其转移概率，并且将CVR预测建模为了图分类任务。







> 推荐系统中常用的两种优化损失函数的机器学习范式：pointwise loss和pairwise loss。
>
> Pointwise 方法
>  Pointwise方法是通过近似为回归问题解决排序问题，输入的单条样本为得分-文档，将每个查询-文档对的相关性得分作为实数分数或者序数分数，使得单个查询-文档对作为样本点(Pointwise的由来)，训练排序模型。预测时候对于指定输入，给出查询-文档对的相关性得分。
>
> pairwise loss :
>  最小化预测输出与目标值之间的平分损失，具体处理是在处理负样本时：把未观察到的实体（即user与item没有交互）当作负样本，或者从未观察到的实体中采样负样本。
>
> Pairwise方法
>  Pairwise方法是通过近似为分类问题解决排序问题，输入的单条样本为标签-文档对。对于一次查询的多个结果文档，组合任意两个文档形成文档对作为输入样本。即学习一个二分类器，对输入的一对文档对AB（Pairwise的由来），根据A相关性是否比B好，二分类器给出分类标签1或0。对所有文档对进行分类，就可以得到一组偏序关系，从而构造文档全集的排序关系。该类方法的原理是对给定的文档全集S，降低排序中的逆序文档对的个数来降低排序错误，从而达到优化排序结果的目的。
>
> pairwise loss :
>  最大化观察到的（即正样本）预测输出和未观察到的（负样本）的预测输出的边缘，表现为观察到的实体得分排名高于未观察到的实体。



## 论文介绍

### 3.1 Graph Convolutional Neural Networks for Web-Scale Recommender Systems[[46\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_46) [PinSage]，KDD 2018，Pinterest

- **问题背景**：现有的GNN难以在大规模图数据场景落地
- **业务场景**：i2i Top N推荐（似乎因为场景复杂性低的问题，这里并没有进一步分召回排序）
- **图的构建**：Pin-Board二分图，Pin是照片，Board类似收藏夹
- **特征使用**：Pin包含了图像特征和文本特征，Board本身没有特征，而是通过Average Pooling对应的Pin们得到
- **采样方法**：Node-Wise Random Walk Sampling，使用个性化PageRank采样邻居，**得分既可以用来加权聚合邻居，也可以用来构造Hard Sample**。
- **Model Architecture**：使用采样时的得分加权聚合邻居

![img](https://pic1.zhimg.com/80/v2-ada50b1d6f58ad690f97d2ca815eb3d8_1440w.jpg)

- **Model Training**

![img](https://pic2.zhimg.com/80/v2-22cbae4c02ad75ed72523d68dec2276d_1440w.png)Max-Margin Loss

- - User在某个Pin下点击的Pin构成一对正例，然后从其他Pin中随机采样一部分作为Easy Negative，**采样时得分位于某个范围的Pin邻居作为Hard Negative**。
  - Easy Sample往往很好区分，模型无法从中学习到有效信息，而**Hard Negative则可以迫使模型学得更好，但是也容易导致收敛困难**，因此可以在起初几个Epoch只使用Easy Sample，然后再逐步加入Hard Sample



### 3.2 Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation[[41\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_41) [MEIRec]，KDD 2019，阿里

- **问题背景**：当前的方法没有充分利用关联信息，作者利用异构图和相应模型来建模学习；通过Term Embedding共享的方法来降低学习量。

- **业务场景**：底纹推荐排序阶段

- **图的构建**：群体用户行为数据构建的Query-Item-User异构图，目标是学习User和Query的Embedding。

- **特征使用**：

- - Query和Item的Title共享Term Embedding，**降低了需要学习的参数量**，**同时可以适应新的Query和Item**
  - User的embedding通过Q-I-U、I-Q-U两条Meta-Path对应的邻居聚合得到
  - User Profile等静态特征最后与GNN得到的Embedding Concat后输入MLP

- **采样方法**：Node-Wise Meta-Path随机采样

- **模型结构**：主要是对于不同类型的邻居采用了不同的Aggregator

- - 对于Item的Query邻居采用Mean Aggregator
  - 对于User的Item和Query邻居采用LSTM Aggregator，考虑到了User对Item和Query的行为是有时序的
  - 对于Query的邻居Item和User采用了CNN Aggregator

![img](https://pic3.zhimg.com/80/v2-5fba9ee88a2c2115599c78e365c1c4c6_1440w.jpg)



### 3.3 IntentGC： a Scalable Graph Convolution Framework Fusing Heterogeneous Information for Recommendation[[45\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_45) [IntentGC]，KDD 2019，阿里

- **问题背景**：已有的工作大多利用社交网络或共现关联分别为User-Item二分图中的Users和Items扩充内部连接，却忽略了属性关联这一类丰富的信息。

![img](https://pic3.zhimg.com/80/v2-09990e2e46efb2d4e343f8fabaa70b76_720w.jpg)

- **业务场景**：广告推荐召回阶段

- **图的构建**：群体用户行为数据+属性数据构建以User和Item为主体的异构图，接着通过User-Property-User和Item-Property-Item构建User-Item异构图，属性结点的类型决定了构建的边的类型。

- **特征使用**：双塔结构，可以用多种特征（不存在特征对齐的问题）

- **采样方法**：先采样一些User-Item Pairs（包括负样本）作为mini-batch，然后对这些User和Item分别Node-Wise Sampling同构邻居。

- **Faster Convolutional Network: IntentNet**

- - **Vector-wise convolution operation**

![img](https://pic3.zhimg.com/80/v2-0a76b6110f08ca62572074dce026f1aa_720w.png)

![img](https://pic3.zhimg.com/80/v2-ed833c51c56b81bf728763e1f6d0e73a_720w.png)

- - 公式（2）有两个作用，一是以不同的重要性融合自身和邻居信息，二是concat后的各维度间的特征交叉，作者认为自身Embedding和邻居Embedding之间的特征交叉没有意义，内部的特征交叉才是有意义的

![img](https://pic2.zhimg.com/80/v2-14fae04ac730e0529a01f1233b82c871_720w.jpg)

- - 公式（3）对公式（2）进行了简化（时间复杂度和模型参数量都有所降低），在vector-wise的层次以不同重要性融合自身和邻居信息，并且通过多组融合操作得到不同角度的信息（类似多个不同的卷积核），最终再进行一次加权融合。

![img](https://pic4.zhimg.com/80/v2-b4872dd0d6b526fd8f67af3a30933723_720w.jpg)

- - **IntentNet**：再经过一个多层MLP进行特征交叉
  - **Heterogeneous relationships**：将Vector-wise convolution operation推广到了存在多种类型关系的情形，对不同类型关系对应的邻居分别聚合然后对得到的邻域表征再加权融合

![img](https://pic2.zhimg.com/80/v2-39519a9aa914e9b8b2b20069f32042b5_720w.png)

- - **Dual Graph Convolution in HIN**：对于User-User和Item-Item分别应用上述模型（双塔结构），最终得到User和Item Embedding做点积，使用Pair-Wise Margin Loss训练。

![img](https://pic4.zhimg.com/80/v2-2f28b4641cb245361c428775f7297f97_720w.png)



### 3.5 M2GRL: A Multi-task Multi-view Graph Representation Learning Framework for Web-scale Recommender Systems[[52\]](https://zhuanlan.zhihu.com/p/423342532?utm_medium=social&utm_oi=56371995213824#ref_52) [M2GRL]，KDD 2020，阿里

- **问题背景**：单个向量表达Multi-View（多种特征空间）信息可能会存在两点问题，容量不足以及分布不同
- **业务场景**：商品推荐召回阶段
- **图的构建**：文中没有具体谈到，应该是群体行为数据构建的多个Single-View同构图。但是值得注意的是，**本文具体实现时并没有用图结构，而是类似Item2Vec的方法直接对行为序列用skip-gram建模**。本文构建了三个Single-View图，基于Item本身构建的图，基于Item Shop构建的图，基于Item-Category构建的图

![img](https://pic1.zhimg.com/80/v2-00bdea5b055d98b519dc5c20e7d43e00_720w.jpg)

- **Node sequence sampling**：用于下文的Skip-Gram方法，**通过Session划分尽量保持滑动窗口内Item的同构性**

- - **Data Clean：**去掉停留时间比较短的Item，用户可能是误点并不感兴趣
  - **Session split and merge**：用户打开和关闭淘宝的时间段作为一个Session，对于时间较长的Session（几个小时，可能是后台运行）进一步拆分，对于时间较短（小于30分钟）的两个连续的Session进行合并

- **Intra-view Representation Learning**：随机采样上文划分后的Session序列，然后使用Skip-Gram方法训练。

![img](https://pic3.zhimg.com/80/v2-434bc3e446535c02fbfd2a2c07d76c52_720w.jpg)

- **Inter-view Alignment**：不同View的信息存在关联，例如，某个Item属于某类Category或者某个Shop，这里进一步学习该关联使得相关的Item-Category或者Item-Shop的Embedding更加接近，使用了一个变换矩阵期望将不同View的Embedding映射到同一个特征空间。

![img](https://pic4.zhimg.com/80/v2-2be075a985d05747ff474c77b0195ab3_720w.jpg)

- **Learning Task Weights with HomoscedasticUncertainty**：考虑到许多任务联合训练，人工设置加权Loss不太现实，这里利用homoscedastic uncertainty来自动学习不同任务的重要性，最终不确定性越大（可学习的参数）的任务权重越低。

![img](https://pic2.zhimg.com/80/v2-1ece718cee8cc18276e2f7300a613c19_720w.jpg)

石塔西解析：https://zhuanlan.zhihu.com/p/286168931

**M2GRL的馅**

M2GRL说以上的学习方式，只是在item, shop, category三个view中**独立学习(“亲兄弟明算账”)**，即所谓的**intra-view task learning**，每个view都遵循标准的word2vec训练

![img](https://pic2.zhimg.com/80/v2-97bb36b1cc8b1b5205059be4721bf9b5_720w.jpg)

M2GRL又说，**只有三个独立的intra-view task learning还不够，还需要加入item-shop, item-category两个cross-view task learning**。这个cross-view task学习什么？论文里的符号也没有交待清楚。根据我的理解，**比如某个item "i"是属于category “c”，那么item i's embedding "![[公式]](https://www.zhihu.com/equation?tex=e_i)"映射到category embedding空间后，就要与自己所携带的category "c"的embedding “![[公式]](https://www.zhihu.com/equation?tex=e_c)”非常近**。(但是论文中的![[公式]](https://www.zhihu.com/equation?tex=W_%7Bic%7D)是怎么回事？如果说![[公式]](https://www.zhihu.com/equation?tex=W_%7Bic%7D+%5Ccdot+e_i)是将item embedding映射到category embedding，那![[公式]](https://www.zhihu.com/equation?tex=e_c)为什么也要映射？如果说是将![[公式]](https://www.zhihu.com/equation?tex=e_i%2Ce_c)映射到第三空间，那为什么作者觉得item embedding与category embedding可以共享一个映射矩阵![[公式]](https://www.zhihu.com/equation?tex=W_%7Bic%7D)？)

![img](https://pic3.zhimg.com/80/v2-99df223838f99da1b0056b8cc0a44b72_720w.jpg)

目前，这里有item/shop/category三个intrra-view task，还有item-shop、item-category两个cross-view task，多个目标一同优化，属于**multi-task learning**。M2GRL还使用了一种fancy的方法来自动确定多个task的权重。公式有点复杂，感觉也不是多重要的问题，就没有仔细看。

![img](https://pic3.zhimg.com/80/v2-05e08573145d0c889340a93749616bea_720w.jpg)

That's it? 基本上是吧，**M2GRL的“馅”都在这里了**，再咬就咬到手指头了。

怎么评价M2GRL？我丝毫不怀疑 M2GRL给作者的项目带来了实实在在的收益，**但是它的馅太小，不对我的口味**。

**本文的馅在哪里？**

M2GRL的馅不对我口味，那么在这个所谓“multi-view多信息融合"的场景下，你能做出什么好吃的包子来吗？嗯，文章的最后，提出一些建设性的意见。

文中的场景被称为multi-view，即item/shop/category是三个view。换一套语境，同样的场景可以被称为异构图（item/shop/category是三类异构节点）或知识图谱（item/shop/category是三类entity）。拿“异构图”或“知识图谱”来搜索，其实业内有很多工作聚焦于"多域信息融合"的问题。

**常规的intra-view task learning**

如果你的项目草创，需要尽快上线一个召回算法。只有intra-view task learning的方案，即通过DeepWalk学习到item/shop/category embedding，再将与用户历史相似的item、相似shop下的item、相似category下的item，作为召回结果返回，其实就已经很不错了，足以满足你项目初期的需求。各个view各做各的，也没有multi-task learning的麻烦。



















