# 你真的懂i2i、u2u 吗？

本文不涉及算法逻辑（弱弱）

i2i：人话item-to-item

u2u：人话user-to-user

又不止有i2i，还有u2i，u2u2i（说法不同，目标一致：都是为了给user 推荐item）

u2u2i：人话user-to-user-to-item（跟图的二阶邻居那样）

搜推广的同学不会陌生的缩写

### i2i 、u2u 思想

在介绍 i2i 、u2u 的前，先介绍推荐系统的小鼻祖——协同过滤



#### 协同过滤

协同过滤（collaborative filtering，简称CF（不是穿越火线））核心思想在于物以类聚，人以群分

物以类聚：User-CF

人以群分：Item-CF

即根据不同的种类进行 CF

举两个简单的例子

> User-CF：杜兰特和哈登是好朋友，都喜欢打篮球和逛xx，那么协同过滤就会认为杜兰特和哈登的相似度很高啊，就会将杜兰特喜欢的球鞋也推给哈登，那么反之，哈登喜欢的xx，也会推荐给杜兰特。（这就是User-CF 的思想）u2u2i

> Item-CF：杜兰特收到了哈登喜欢的xx 后，杜兰特也去了xx，那么推荐系统知道了呀（你们这些坏家伙，我懂了），然后推荐系统就把和 xx 相似度很高的（可能地理位置一样，音乐氛围一样...）推给了杜兰特。（这就是Item-CF）

受伤的永远是杜兰特

一图胜千言

u2u2i

![f28c2c6ecfcb1977.png](https://i.bmp.ovh/imgs/2022/08/11/f28c2c6ecfcb1977.png)

i2i

![804da9eb449804ad.png](https://i.bmp.ovh/imgs/2022/08/11/804da9eb449804ad.png)

#### 什么是 i2i、u2u？

那么上述提到的协同过滤只是个抽象的形式，并没有细致到相似度如何计算等细节

而i2i 的核心思想，就是Item-CF

根据用户行为的正样本，寻找与正样本“相似的”的item 来做推荐

举个例子：

基于文本相似度/内容 的i2i（item-to-item），可以叫做 content i2i

基于标签的i2i，根据标签进行建拉链，可以叫做tag i2i

（当然你想怎么叫都行，直接叫i2i 抽象所有细节也行）

基于Graph Embedding 的i2i，可以叫Graph i2i



既然讲到了Graph Embedding，那就再深入一点点，讲 i2i 、u2u 的向量召回方法



### u2u 向量召回

什么是 embedding？（用于表示一个东西）

简单点的例子：男-1，女-0，年龄是[21]，那属于作者的embedding [1, 21]

当然这是简单的不能再简单的表示了，就是用多维向量表示user/item/else（毕竟这篇文章也不是讲embedding  :）

又到了快乐的举例子时候了：

这个时候呢，哈登和杜兰特都有专属于他们的表示了

> 哈登：Harden_emb = [30, 195, 125, 1, 1]   30岁，195cm，125kg，1->喜欢打篮球，反之，1-> 喜欢去xx
>
> 杜兰特：Durant_emb = [32, 210, 135, 1 , 1]  32岁，210cm，135kg，1->喜欢打篮球，反之，1-> 喜欢去xx

两者数据相似度怎么衡量呢？

就不过多介绍了

1. cos 余弦相似度
2. 内积
3. fassic
由于工业级数据庞大，第一种和第二种的复杂度过高，所以第三种方法广泛运用在工业界上（精度下降一点点）



简单来说，就是

> similar(Harden, Durant) = f(Harden_emb, Durant_emb)
>
> f 表示三种方法其中之一

Hold on Hold on 刚刚讲的是u2u了，因为是计算用户相似度，再用来做用户召回



### i2i 向量召回

这时候item 可以抽象为 哈登喜欢去的 xx （bar）~

> A_Bar：A_Bar_emb = [30, 1, 260, 190]  有30个mm，1代表在布鲁克林，260美元入场票，1900代表1900 平方的的bar（够大）
>
> B_Bar：B_Bar_emb = [25, 1, 250, 180]  同上

那么通过u2u 向量召回讲的相似度计算，A_Bar 根据这种相似度计算，从全美国的Bar 里面找，找到了B_Bar 的 embedding 相似度最高

那i2i 其实也可以叫 b2b（bar-to-bar），通过A trigger（找到） B

然后再推荐给哈登

![804da9eb449804ad.png](https://i.bmp.ovh/imgs/2022/08/11/804da9eb449804ad.png)





工业进行产生用于向量召回的模型：DSSM、Wide && Deep、item2Vec、DeepWalk、GraphSAGE（层出不穷）

### i2i 倒排索引

倒排索引就是对向量召回的一种离线优化，避免每次在线请求都要去计算杜兰特和哈登的相似度、A_Bar 和 B_Bar 的相似度

官方一点说

> 通过item embedding，找到与本item 相似的其它item，离线构建i2i 索引。线上使用时，通过用户历史行为的item 作为trigger，从倒排索引中找到候选集

人话

> 离线计算所有item、user，存起来
>
> 倒排索引["A_Bar"] = ["B_Bar", "C_Bar" ... ] 
>
> 如果发现杜兰特去完A_Bar，想去A bar 相似的bar，那就直接去请求索引服务，拿到倒排索引就可以推荐相似的啦

### 工业界实践

那么问题来了，我怎么去定义杜兰特去过的酒吧跟酒吧的相似度呢？

我是拿杜兰特十年前去过酒吧、还是拿最近去过的做 i2i

我是拿杜兰特十年前的老朋友、还是拿最近的好朋友做u2u

简单的做法是时间序，拿最近去过的酒吧时间序，最近的好朋友的时间序

比如：

最近十天的酒吧作为trigger 酒吧

最近一个月的朋友作为trigger 朋友

这样

#### session 内的 i2i

那么在一个平台上，比如淘系的场景

一个用户在10分钟内在平台的点击都算一个session 内的点击（假设10分钟作为一个session）

那么淘宝可能在某些召回分支（如：MIND等）

拿最近时间内的item 作为trigger item 去请求MIND 模型，拿到对应trigger item 相似度最高的item

> 比如拿100 个item 作trigger，每个item trigger 2个回来，那就是200 个item 作为召回结果

这就是i2i 的一个做法



### 总结

i2i 不是一个难理解的概念，它是一个抽象一点的概念（类似CF（协同过滤）一样）

有人说i2i 不是一个考虑人的因素，我觉得是不严谨的说话

1. i2i 的embedding trigger 可以是 u-i 模型训练出来，再去作i2i
所以可能，在i2i 的相似度计算模型应该是单纯利用了item 的信息进行计算得出embedding ，再做i2i 才能这样说

> 第一次写这种文章，大佬别喷... 
>
> 评论指正，小弟立马改



参考：

https://zhuanlan.zhihu.com/p/475120402