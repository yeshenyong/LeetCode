# 大模型主流架构文档









### QWen

QWen 系列学习
1. 与LLaMa2 算法架构基本一致
2. QWen-72B/1.8B 3T tokens and support 32k contex
3. https://qwenlm.github.io/blog/qwen1.5/
    a. 开源了包括 0.5B、1.8B、4B、7B、14B 和 72B 在内的 6 种大小的 Base 和 Chat 模型，并且我们还提供了量化模型
    b. vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地 LLM 推理） LlamaIndex、LangChain、CrewAI
    c. 尽管上述结果仅突显了我们在处理 32K tokens 长度时所展现的卓越性能，但这并不代表模型的最大支持长度仅限于 32K。您可以在 config.json 中，将 max_position_embedding 和 sliding_window 尝试修改为更大的值，观察模型在更长上下文理解场景下，是否可以达到您满意的效果。
    d. RAG、Code Interpreter、Agent
4. https://qwen.readthedocs.io/en/latest/
https://github.com/oobabooga/text-generation-webui