# 大模型数据集涵盖

撰写ing



### SFT

#### alpaca data set

meta开源了他们的LLaMA系列模型，包含了参数量为7B/13B/33B/65B的不同模型，然而由于原模型未经过instruct tuning，因此体现出的效果较差。基于以上原因，standford的一个团队推出了stanford_alpaca项目，该项目提供了廉价的对llama模型微调方法——利用openai提供的gpt

【generated by text-davinci-003 
code-davinci-002是一个基础模型，适用于纯代码补全任务。
text-davinci-002是基于code-davinci-002的InstructGPT模型。
text-davinci-003是text-davinci-002的改进版。
gpt-3.5-turbo-0301是在text-davinci-003的基础上进行了改进，针对聊天应用进行了优化】

模型api生成质量较高的instruct tuning数据（仅52k），并且基于这些数据微调模型。原repo中他们使用了4张A100显卡训练3h即可完成微调，经过调整训练参数和采用更高效的框架后，甚至可以采用V100乃至3090这种显存更小的显卡进行微调。（目前已有其他同样便捷的微调方式，如alpaca-lora）。实际上repo中所包含的alpaca_data.json 即是他们训练所用的数据集，我们可以直接使用该数据集进行模型微调。但是在alpaca-lora中提到该数据集存在一些noise，因此他们对数据集做了清洗后得到alpaca_data_cleaned.json，理论上采用这一数据集进行训练可以得到更好结果



